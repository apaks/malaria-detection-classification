{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fifty-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to use cellpose to segment cells\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, os, sys, glob\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from scipy.ndimage import find_objects\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "from cellpose import models, utils, io\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from cellpose.utils import outlines_list\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "path = r\"C:\\Users\\AChub_Lab\\Desktop\\my_project\\DMSO\\31_3_21 WT_K13 images\"\n",
    "os.chdir(path)\n",
    "for file in glob.glob(\"*.jpeg\"):\n",
    "    files.append(os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will download images from website\n",
    "# urls = ['http://www.cellpose.org/static/images/img02.png',\n",
    "#         'http://www.cellpose.org/static/images/img03.png',\n",
    "#         'http://www.cellpose.org/static/images/img05.png']\n",
    "# files = []\n",
    "# for url in urls:\n",
    "#     parts = urlparse(url)\n",
    "#     filename = os.path.basename(parts.path)\n",
    "#     if not os.path.exists(filename):\n",
    "#         sys.stderr.write('Downloading: \"{}\" to {}\\n'.format(url, filename))\n",
    "#         utils.download_url_to_file(url, filename)\n",
    "#     files.append(filename)\n",
    "\n",
    "# REPLACE FILES WITH YOUR IMAGE PATHS\n",
    "\n",
    "\n",
    "# files = ['img0.tif', 'img1.tif']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-fancy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "specialized-andrew",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "enormous-monthly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "path = r\"C:\\Users\\AChub_Lab\\Desktop\\my_project\\DMSO\\DMSO\\test\\images\"\n",
    "matches = []\n",
    "for root, dirnames, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(('.tif')):\n",
    "#             print (filename)\n",
    "            matches.append(os.path.join(root, filename))\n",
    "print (len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches[80:82]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tracemalloc\n",
    "filename = r\"C:\\Users\\AChub_Lab\\Desktop\\my_project\\DMSO\\DMSO\\T0D2_1.tif\"\n",
    "model = models.Cellpose(gpu=True, model_type ='cyto')\n",
    "tracemalloc.start()\n",
    "img = io.imread(filename)\n",
    "masks, flows, styles, diams = model.eval(img, \n",
    "                            batch_size = 8,\n",
    "                         diameter= diam, \n",
    "                         channels=chan,\n",
    "                         invert = True,\n",
    "                         flow_threshold = 0,\n",
    "                         cellprob_threshold = -4,\n",
    "                                        )\n",
    "\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-individual",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN CELLPOSE\n",
    "\n",
    "\n",
    "\n",
    "# DEFINE CELLPOSE MODEL\n",
    "# model_type='cyto' or model_type='nuclei'\n",
    "model = models.Cellpose(gpu=True, model_type ='cyto')\n",
    "\n",
    "# define CHANNELS to run segementation on\n",
    "# grayscale=0, R=1, G=2, B=3\n",
    "# channels = [cytoplasm, nucleus]\n",
    "# if NUCLEUS channel does not exist, set the second channel to 0\n",
    "# channels = [0,0]\n",
    "# IF ALL YOUR IMAGES ARE THE SAME TYPE, you can give a list with 2 elements\n",
    "channels = [[0,0]] * len(matches) # IF YOU HAVE GRAYSCALE\n",
    "# channels = [2,3] # IF YOU HAVE G=cytoplasm and B=nucleus\n",
    "# channels = [2,1] # IF YOU HAVE G=cytoplasm and R=nucleus\n",
    "\n",
    "# or if you have different types of channels in each image\n",
    "# channels = [[0,0], [0,0], [0,0]]\n",
    "\n",
    "# if diameter is set to None, the size of the cells is estimated on a per image basis\n",
    "# you can set the average cell `diameter` in pixels yourself (recommended) \n",
    "# diameter can be a list or a single number for all images\n",
    "\n",
    "# you can run all in a list e.g.\n",
    "# >>> imgs = [io.imread(filename) in files]\n",
    "# >>> masks, flows, styles, diams = model.eval(imgs, diameter=None, channels=channels)\n",
    "# >>> io.masks_flows_to_seg(imgs, masks, flows, diams, files, channels)\n",
    "# >>> io.save_to_png(imgs, masks, flows, files)\n",
    "diam = 100\n",
    "# or in a loop\n",
    "for chan, filename in zip(channels, matches[:]):\n",
    "    name, file_extension = os.path.splitext(filename)\n",
    "    filename = r\"C:\\Users\\AChub_Lab\\Desktop\\my_project\\DMSO\\gametocytes\\Disk 1\\NF54-R-110120-7DAY-3.jpg\"\n",
    "#     if os.path.exists(name + \"_seg.npy\"):\n",
    "#         continue\n",
    "#     else:\n",
    "    print (filename)\n",
    "    try:\n",
    "        img = io.imread(filename)\n",
    "        masks, flows, styles, diams = model.eval(img, \n",
    "                                 diameter= diam, \n",
    "                                 channels=chan,\n",
    "                                 invert = True,\n",
    "                                 flow_threshold = 1,\n",
    "                                 cellprob_threshold = -4,\n",
    "                                                )\n",
    "\n",
    "        # save results so you can load in gui\n",
    "#         io.masks_flows_to_seg(img, masks, flows, diams, filename, chan)\n",
    "\n",
    "        # save results as png\n",
    "#         io.save_to_png(img, masks, flows, filename)\n",
    "    except Exception as err:\n",
    "        print (err)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = io.imread(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_img = img.copy()\n",
    "outlines_ls = outlines_list(masks)\n",
    "for idx, cell in enumerate(outlines_ls[:]):\n",
    "    # get crop\n",
    "    out = get_crop_from_outline(cell, tmp_img)\n",
    "    if out.shape[0] < size_thres or out.shape[1] < size_thres:\n",
    "        continue\n",
    "        # find purple patches to save only infected cells\n",
    "    hsv = cv2.cvtColor(out, cv2.COLOR_RGB2HSV)\n",
    "    lower_range = np.array([120,50,50])\n",
    "    upper_range = np.array([150,255,255])\n",
    "    color_mask = cv2.inRange(hsv, lower_range, upper_range)\n",
    "    pixel_sum = color_mask.sum()\n",
    "\n",
    "    if pixel_sum > 5e4:\n",
    "        print (pixel_sum)\n",
    "        f, ax = plt.subplots(1, 2, figsize=(4,4))\n",
    "        ax[0].imshow(out)\n",
    "        ax[1].imshow(color_mask)\n",
    "        ax[0].axis('off')\n",
    "#         ax[1].axis('off')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAY RESULTS\n",
    "from cellpose import plot\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "plot.show_segmentation(fig, img, masks, flows[0], channels=chan)\n",
    "plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = []\n",
    "path = r\"C:\\Users\\AChub_Lab\\Desktop\\my_project\\DMSO\\DMSO\"\n",
    "os.chdir(path)\n",
    "for file in glob.glob(\"*.txt\"):\n",
    "    txt_files.append(os.path.join(path, file))\n",
    "print (len(txt_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-potato",
   "metadata": {},
   "source": [
    "##### Save individual crops from cellpose output txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in txt_files[2::5]:\n",
    "    # get sample ifo\n",
    "    foldername, file_extension = os.path.splitext(fname)\n",
    "    foldername = foldername.split(\"_cp\")[0]\n",
    "    # load cell outlines \n",
    "    file1 = open(fname,\"r+\")\n",
    "    ls_outlines = file1.readlines()\n",
    "    file1.close() \n",
    "    # load corresp image\n",
    "    img_path = foldername + \".tif\"\n",
    "    print(img_path)\n",
    "    img = io.imread(img_path)\n",
    "    foldername = foldername + \"_un\"\n",
    "    # create folder to save crops\n",
    "    if not os.path.exists(foldername):\n",
    "        os.makedirs(foldername)\n",
    "    \n",
    "    for idx, cell in enumerate(ls_outlines[::10]):\n",
    "        # get coordinates of the contour\n",
    "        x = cell[:-1].split(\",\")[::2]\n",
    "        x = [int(i) for i in x]\n",
    "        y = cell[:-1].split(\",\")[1::2]\n",
    "        y = [int(i) for i in y]\n",
    "        # create list of tuples for cv2\n",
    "        out_pix = np.array(list(zip(x, y)))\n",
    "        \n",
    "        # mask outline\n",
    "        mask = np.zeros(img.shape, dtype=np.uint8)\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,)*channel_count\n",
    "        # fill contour\n",
    "        cv2.fillConvexPoly(mask, out_pix, ignore_mask_color)\n",
    "\n",
    "        masked_image = cv2.bitwise_and(img, mask)\n",
    "        \n",
    "        # crop the box around the cell\n",
    "        (topy, topx) = (np.min(y), np.min(x))\n",
    "        (bottomy, bottomx) = (np.max(y), np.max(x))\n",
    "        out = masked_image[topy:bottomy+1, topx:bottomx+1,:]\n",
    "        \n",
    "        # find purple patches to save only infected cells\n",
    "        hsv = cv2.cvtColor(out, cv2.COLOR_RGB2HSV)\n",
    "        lower_range = np.array([120,50,50])\n",
    "        upper_range = np.array([150,255,255])\n",
    "        color_mask = cv2.inRange(hsv, lower_range, upper_range)\n",
    "        pixel_sum = color_mask.sum()\n",
    "\n",
    "    #     plt.plot(x, y)\n",
    "        if pixel_sum > 3e4:\n",
    "            print (pixel_sum)\n",
    "#             f, ax = plt.subplots(1, 2, figsize=(4,4))\n",
    "#             ax[0].imshow(out)\n",
    "#             ax[1].imshow(color_mask)\n",
    "#             ax[0].axis('off')\n",
    "#             ax[1].axis('off')\n",
    "#             plt.show()\n",
    "#             plt.savefig(os.path.join(foldername, os.path.basename(name) + str(idx) + \".png\" ))\n",
    "#             cv2.imwrite(os.path.join(name, str(idx) + \".png\" ), out)\n",
    "#             im = Image.fromarray(out)\n",
    "#             im.save(os.path.join(foldername, os.path.basename(foldername) + str(idx) + \".png\" ))\n",
    "        else:\n",
    "            im = Image.fromarray(out)\n",
    "            im.save(os.path.join(foldername, os.path.basename(foldername) + str(idx) + \".png\" ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-convert",
   "metadata": {},
   "source": [
    "#### Save ind crops from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.splitext(filename)[0].split('\\\\')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN CELLPOSE\n",
    "model = models.Cellpose(gpu=True, model_type ='cyto')\n",
    "channels = [[0,0]] * len(matches) # IF YOU HAVE GRAYSCALE\n",
    "diam = 100\n",
    "size_thres = diam*0.6\n",
    "\n",
    "# or in a loop\n",
    "for chan, filename in zip(channels, matches[:10]):\n",
    "    print('*'*10, filename.split('\\\\')[-1], '*'*10)\n",
    "    foldername, file_extension = os.path.splitext(filename)\n",
    "\n",
    "    print (foldername)\n",
    "    if not os.path.exists(foldername):\n",
    "        os.makedirs(foldername)\n",
    "    try:\n",
    "        img = io.imread(filename)\n",
    "        masks, flows, styles, diams = model.eval(img, \n",
    "                                 diameter= diam, \n",
    "                                 channels=chan,\n",
    "                                 invert = True,\n",
    "#                                  flow_threshold = 1,\n",
    "#                                  cellprob_threshold = -4,\n",
    "                                                )\n",
    "\n",
    "    except Exception as err:\n",
    "        print (err)\n",
    "        \n",
    "\n",
    "    since = time.time()\n",
    "    d_results = {\"un\": [], \"ring\": [], \"troph\": [], \"shiz\": [] }\n",
    "    \n",
    "#     outlines = utils.masks_to_outlines(masks)\n",
    "    \n",
    "#     fig, ax = plt.subplots(figsize = (12, 12))                   \n",
    "#     outX, outY = np.nonzero(outlines)\n",
    "#     imgout = img.copy()\n",
    "#     imgout[outX, outY] = np.array([0,0,0])\n",
    "#     ax.imshow(imgout)\n",
    "#     ax.set_title('predicted outlines')\n",
    "#     ax.axis('off')\n",
    "#     plt.show()\n",
    "    tmp_img = img.copy()\n",
    "    outlines_ls = outlines_list(masks)\n",
    "    for idx, cell in enumerate(outlines_ls[:]):\n",
    "        # get crop\n",
    "        out = get_crop_from_outline(cell, tmp_img)\n",
    "        if out.shape[0] < size_thres or out.shape[1] < size_thres:\n",
    "            continue\n",
    "            # find purple patches to save only infected cells\n",
    "        hsv = cv2.cvtColor(out, cv2.COLOR_RGB2HSV)\n",
    "        lower_range = np.array([120,50,50])\n",
    "        upper_range = np.array([150,255,255])\n",
    "        color_mask = cv2.inRange(hsv, lower_range, upper_range)\n",
    "        pixel_sum = color_mask.sum()\n",
    "        \n",
    "        if pixel_sum > 5e3:\n",
    "            print (pixel_sum)\n",
    "#             f, ax = plt.subplots(1, 2, figsize=(4,4))\n",
    "#             ax[0].imshow(out)\n",
    "#             ax[1].imshow(color_mask)\n",
    "#             ax[0].axis('off')\n",
    "#             ax[1].axis('off')\n",
    "            im = Image.fromarray(out)\n",
    "            im.save(os.path.join(foldername, os.path.basename(foldername) + '_' +  str(idx) + \".png\" ))\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.basename(os.path.splitext(filename)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlines = utils.masks_to_outlines(masks)\n",
    "\n",
    "# plot the WordCloud image     \n",
    "fig, ax = plt.subplots(figsize = (12, 12))                   \n",
    "outX, outY = np.nonzero(outlines)\n",
    "imgout = img.copy()\n",
    "imgout[outX, outY] = np.array([0,100,0])\n",
    "ax.imshow(imgout)\n",
    "#for o in outpix:\n",
    "#    ax.plot(o[:,0], o[:,1], color=[1,0,0], lw=1)\n",
    "ax.set_title('predicted outlines')\n",
    "ax.axis('off')\n",
    "# ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_txt = r\"C:\\Users\\AChub_Lab\\Desktop\\my_project\\DMSO\\DMSO\\T32D2_1_cp_outlines.txt\"\n",
    "file1 = open(path_txt,\"r+\")\n",
    "outlines = file1.readlines()\n",
    "\n",
    "file1.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell in outlines:\n",
    "    x = cell[:-1].split(\",\")[::2]\n",
    "    x = [int(i) for i in x]\n",
    "    y = cell[:-1].split(\",\")[1::2]\n",
    "    y = [int(i) for i in y]\n",
    "    out_pix = list(zip(x, y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-service",
   "metadata": {},
   "source": [
    "### Prepare individual crops for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "radio-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "apart-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(arr):\n",
    "    my_transforms = transforms.Compose([\n",
    "                                    transforms.Resize(255),\n",
    "                                    transforms.CenterCrop(224),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(\n",
    "                                        [0.485, 0.456, 0.406],\n",
    "                                        [0.229, 0.224, 0.225])])\n",
    "#     image = Image.open(io.BytesIO(image_bytes))\n",
    "    im = Image.fromarray(arr)\n",
    "    return my_transforms(im).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "marine-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"U\", 'R', 'T', 'S']\n",
    "\n",
    "def get_prediction(arr):\n",
    "    tensor = transform_image(arr)\n",
    "    outputs = pred_model.forward(tensor)\n",
    "    _, y_hat = outputs.max(1)\n",
    "    return class_names[y_hat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-wednesday",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-surveillance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "specific-harvard",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "# Load\n",
    "PATH = r\"C:\\Users\\AChub_Lab\\Desktop\\my_project\\st_app\\malaria-detection-classification\\model.pth\"\n",
    "pred_model = torch.load(PATH, map_location = device)\n",
    "pred_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-trick",
   "metadata": {},
   "source": [
    "#### Approach 1: outlines list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "solid-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_from_outline(cell, tmp_img):\n",
    "    # get xy coords\n",
    "    x = cell.flatten()[::2]\n",
    "    y = cell.flatten()[1::2]\n",
    "    # remove cells on the border\n",
    "    if 0 in x or 0 in y or tmp_img.shape[0] - 1 in y or tmp_img.shape[1] - 1 in x:\n",
    "        return 0\n",
    "    # mask outline\n",
    "    mask = np.zeros(tmp_img.shape, dtype=np.uint8)\n",
    "    channel_count = tmp_img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "    ignore_mask_color = (255,)*channel_count\n",
    "    # fill contour\n",
    "    cv2.fillConvexPoly(mask, cell, ignore_mask_color)\n",
    "    masked_image = cv2.bitwise_and(tmp_img, mask)\n",
    "    # crop the box around the cell\n",
    "    (topy, topx) = (np.min(y), np.min(x))\n",
    "    (bottomy, bottomx) = (np.max(y), np.max(x))\n",
    "    out = masked_image[topy:bottomy+1, topx:bottomx+1,:]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-native",
   "metadata": {},
   "source": [
    "### Run segmentation and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "intellectual-gateway",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** TORCH CUDA version installed and working. **\n",
      ">>>> using GPU\n",
      "********** T0R1_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 0.96s; flow+mask computation 1.47\n",
      "estimated masks for 1 image(s) in 2.68 sec\n",
      ">>>> TOTAL TIME 2.68 sec\n",
      "Inference complete in 0m 10s\n",
      "********** T10R2_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 0.97s; flow+mask computation 2.18\n",
      "estimated masks for 1 image(s) in 3.42 sec\n",
      ">>>> TOTAL TIME 3.42 sec\n",
      "Inference complete in 0m 16s\n",
      "********** T10R2_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.00s; flow+mask computation 2.02\n",
      "estimated masks for 1 image(s) in 3.26 sec\n",
      ">>>> TOTAL TIME 3.26 sec\n",
      "Inference complete in 0m 15s\n",
      "********** T12R3_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 0.98s; flow+mask computation 1.52\n",
      "estimated masks for 1 image(s) in 2.77 sec\n",
      ">>>> TOTAL TIME 2.77 sec\n",
      "Inference complete in 0m 10s\n",
      "********** T12R3_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 0.97s; flow+mask computation 1.56\n",
      "estimated masks for 1 image(s) in 2.79 sec\n",
      ">>>> TOTAL TIME 2.79 sec\n",
      "Inference complete in 0m 11s\n",
      "********** T4R3_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 0.98s; flow+mask computation 2.48\n",
      "estimated masks for 1 image(s) in 3.73 sec\n",
      ">>>> TOTAL TIME 3.73 sec\n",
      "Inference complete in 0m 18s\n",
      "********** T6R2_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.01s; flow+mask computation 2.45\n",
      "estimated masks for 1 image(s) in 3.70 sec\n",
      ">>>> TOTAL TIME 3.70 sec\n",
      "Inference complete in 0m 20s\n",
      "********** T6R3_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 0.95s; flow+mask computation 2.41\n",
      "estimated masks for 1 image(s) in 3.64 sec\n",
      ">>>> TOTAL TIME 3.64 sec\n",
      "Inference complete in 0m 18s\n",
      "********** T8R1_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 0.99s; flow+mask computation 1.95\n",
      "estimated masks for 1 image(s) in 3.19 sec\n",
      ">>>> TOTAL TIME 3.19 sec\n",
      "Inference complete in 0m 14s\n",
      "********** T8R1_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 0.95s; flow+mask computation 2.01\n",
      "estimated masks for 1 image(s) in 3.24 sec\n",
      ">>>> TOTAL TIME 3.24 sec\n",
      "Inference complete in 0m 15s\n",
      "********** T8R2_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 0.97s; flow+mask computation 1.89\n",
      "estimated masks for 1 image(s) in 3.12 sec\n",
      ">>>> TOTAL TIME 3.12 sec\n",
      "Inference complete in 0m 13s\n",
      "********** T8R2_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 0.97s; flow+mask computation 1.95\n",
      "estimated masks for 1 image(s) in 3.17 sec\n",
      ">>>> TOTAL TIME 3.17 sec\n",
      "Inference complete in 0m 13s\n",
      "********** T8R3_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.89s; flow+mask computation 1.86\n",
      "estimated masks for 1 image(s) in 4.00 sec\n",
      ">>>> TOTAL TIME 4.00 sec\n",
      "Inference complete in 0m 9s\n",
      "********** T8R3_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 0.95s; flow+mask computation 1.73\n",
      "estimated masks for 1 image(s) in 2.93 sec\n",
      ">>>> TOTAL TIME 2.93 sec\n",
      "Inference complete in 0m 10s\n",
      "********** T34R1_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 0.96s; flow+mask computation 1.45\n",
      "estimated masks for 1 image(s) in 2.65 sec\n",
      ">>>> TOTAL TIME 2.65 sec\n",
      "Inference complete in 0m 9s\n",
      "********** T34R2_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 0.96s; flow+mask computation 1.87\n",
      "estimated masks for 1 image(s) in 3.09 sec\n",
      ">>>> TOTAL TIME 3.09 sec\n",
      "Inference complete in 0m 10s\n",
      "********** T36R1_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 0.94s; flow+mask computation 2.17\n",
      "estimated masks for 1 image(s) in 3.37 sec\n",
      ">>>> TOTAL TIME 3.37 sec\n",
      "Inference complete in 0m 12s\n",
      "********** T36R2_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 0.96s; flow+mask computation 1.92\n",
      "estimated masks for 1 image(s) in 3.14 sec\n",
      ">>>> TOTAL TIME 3.14 sec\n",
      "Inference complete in 0m 9s\n",
      "********** T38R1_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 0.96s; flow+mask computation 1.37\n",
      "estimated masks for 1 image(s) in 2.60 sec\n",
      ">>>> TOTAL TIME 2.60 sec\n",
      "Inference complete in 0m 8s\n",
      "********** T38R1_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.06s; flow+mask computation 1.86\n",
      "estimated masks for 1 image(s) in 3.24 sec\n",
      ">>>> TOTAL TIME 3.24 sec\n",
      "Inference complete in 0m 12s\n",
      "********** T38R2_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.10s; flow+mask computation 2.69\n",
      "estimated masks for 1 image(s) in 4.14 sec\n",
      ">>>> TOTAL TIME 4.14 sec\n",
      "Inference complete in 0m 14s\n",
      "********** T38R3_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.09s; flow+mask computation 1.75\n",
      "estimated masks for 1 image(s) in 3.16 sec\n",
      ">>>> TOTAL TIME 3.16 sec\n",
      "Inference complete in 0m 11s\n",
      "********** T38R3_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.06s; flow+mask computation 1.87\n",
      "estimated masks for 1 image(s) in 3.24 sec\n",
      ">>>> TOTAL TIME 3.24 sec\n",
      "Inference complete in 0m 12s\n",
      "********** T40R1_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.05s; flow+mask computation 2.33\n",
      "estimated masks for 1 image(s) in 3.69 sec\n",
      ">>>> TOTAL TIME 3.69 sec\n",
      "Inference complete in 0m 17s\n",
      "********** T40R1_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.08s; flow+mask computation 2.64\n",
      "estimated masks for 1 image(s) in 4.03 sec\n",
      ">>>> TOTAL TIME 4.03 sec\n",
      "Inference complete in 0m 19s\n",
      "********** T40R2_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.11s; flow+mask computation 2.10\n",
      "estimated masks for 1 image(s) in 3.51 sec\n",
      ">>>> TOTAL TIME 3.51 sec\n",
      "Inference complete in 0m 14s\n",
      "********** T40R2_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.15s; flow+mask computation 2.19\n",
      "estimated masks for 1 image(s) in 3.65 sec\n",
      ">>>> TOTAL TIME 3.65 sec\n",
      "Inference complete in 0m 16s\n",
      "********** T42R1_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.06s; flow+mask computation 2.87\n",
      "estimated masks for 1 image(s) in 4.26 sec\n",
      ">>>> TOTAL TIME 4.26 sec\n",
      "Inference complete in 0m 20s\n",
      "********** T42R1_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.07s; flow+mask computation 2.17\n",
      "estimated masks for 1 image(s) in 3.56 sec\n",
      ">>>> TOTAL TIME 3.56 sec\n",
      "Inference complete in 0m 14s\n",
      "********** T16R2_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.13s; flow+mask computation 2.49\n",
      "estimated masks for 1 image(s) in 3.95 sec\n",
      ">>>> TOTAL TIME 3.95 sec\n",
      "Inference complete in 0m 20s\n",
      "********** T18R2_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.08s; flow+mask computation 2.78\n",
      "estimated masks for 1 image(s) in 4.17 sec\n",
      ">>>> TOTAL TIME 4.17 sec\n",
      "Inference complete in 0m 22s\n",
      "********** T18R3_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.10s; flow+mask computation 2.36\n",
      "estimated masks for 1 image(s) in 3.76 sec\n",
      ">>>> TOTAL TIME 3.76 sec\n",
      "Inference complete in 0m 18s\n",
      "********** T18R3_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.13s; flow+mask computation 2.47\n",
      "estimated masks for 1 image(s) in 3.90 sec\n",
      ">>>> TOTAL TIME 3.90 sec\n",
      "Inference complete in 0m 18s\n",
      "********** T20R3_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.07s; flow+mask computation 2.10\n",
      "estimated masks for 1 image(s) in 3.47 sec\n",
      ">>>> TOTAL TIME 3.47 sec\n",
      "Inference complete in 0m 16s\n",
      "********** T20R3_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.09s; flow+mask computation 2.15\n",
      "estimated masks for 1 image(s) in 3.56 sec\n",
      ">>>> TOTAL TIME 3.56 sec\n",
      "Inference complete in 0m 16s\n",
      "********** T24R1_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.07s; flow+mask computation 2.34\n",
      "estimated masks for 1 image(s) in 3.74 sec\n",
      ">>>> TOTAL TIME 3.74 sec\n",
      "Inference complete in 0m 17s\n",
      "********** T24R1_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.07s; flow+mask computation 2.14\n",
      "estimated masks for 1 image(s) in 3.52 sec\n",
      ">>>> TOTAL TIME 3.52 sec\n",
      "Inference complete in 0m 15s\n",
      "********** T24R2_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.06s; flow+mask computation 2.57\n",
      "estimated masks for 1 image(s) in 3.97 sec\n",
      ">>>> TOTAL TIME 3.97 sec\n",
      "Inference complete in 0m 19s\n",
      "********** T24R2_2.tif **********\n",
      "processing 1 image(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent: running network 1.06s; flow+mask computation 2.60\n",
      "estimated masks for 1 image(s) in 3.96 sec\n",
      ">>>> TOTAL TIME 3.96 sec\n",
      "Inference complete in 0m 21s\n",
      "********** T26R2_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.08s; flow+mask computation 2.04\n",
      "estimated masks for 1 image(s) in 3.43 sec\n",
      ">>>> TOTAL TIME 3.43 sec\n",
      "Inference complete in 0m 14s\n",
      "********** T26R2_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.11s; flow+mask computation 1.90\n",
      "estimated masks for 1 image(s) in 3.30 sec\n",
      ">>>> TOTAL TIME 3.30 sec\n",
      "Inference complete in 0m 13s\n",
      "********** T26R3_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.11s; flow+mask computation 2.56\n",
      "estimated masks for 1 image(s) in 4.00 sec\n",
      ">>>> TOTAL TIME 4.00 sec\n",
      "Inference complete in 0m 21s\n",
      "********** T28R1_2.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.09s; flow+mask computation 1.96\n",
      "estimated masks for 1 image(s) in 3.35 sec\n",
      ">>>> TOTAL TIME 3.35 sec\n",
      "Inference complete in 0m 14s\n",
      "********** T30R1_1.tif **********\n",
      "processing 1 image(s)\n",
      "time spent: running network 1.11s; flow+mask computation 2.30\n",
      "estimated masks for 1 image(s) in 3.73 sec\n",
      ">>>> TOTAL TIME 3.73 sec\n",
      "Inference complete in 0m 15s\n"
     ]
    }
   ],
   "source": [
    "# RUN CELLPOSE\n",
    "save_path = r\"C:\\Users\\AChub_Lab\\Desktop\\my_project\\DMSO\\DMSO\\test\\crops\"\n",
    "\n",
    "\n",
    "model = models.Cellpose(gpu=True, model_type ='cyto')\n",
    "channels = [[0,0]] * len(matches) # IF YOU HAVE GRAYSCALE\n",
    "diam = 100\n",
    "size_thres = diam*0.8\n",
    "ls_crops = []\n",
    "# or in a loop\n",
    "for chan, filename in zip(channels, matches[1:]):\n",
    "    print('*'*10, filename.split('\\\\')[-1], '*'*10)\n",
    "    img_name = os.path.basename(os.path.splitext(filename)[0])\n",
    "    \n",
    "    try:\n",
    "        img = io.imread(filename)\n",
    "        masks, flows, styles, diams = model.eval(img, \n",
    "                                 diameter= diam, \n",
    "                                 channels=chan,\n",
    "                                 invert = True,\n",
    "                                 flow_threshold = 1,\n",
    "                                 cellprob_threshold = -4,\n",
    "                                                )\n",
    "\n",
    "    except Exception as err:\n",
    "        print (err)\n",
    "        \n",
    "\n",
    "    tmp_img = img.copy()\n",
    "    since = time.time()\n",
    "    d_results = {\"U\": [], \"R\": [], \"T\": [], \"S\": [] }\n",
    "    outlines_ls = outlines_list(masks)\n",
    "    with torch.no_grad():\n",
    "        for idx, cell in enumerate(outlines_ls[:]):\n",
    "            # get crop\n",
    "            out = get_crop_from_outline(cell, tmp_img)\n",
    "            if not isinstance(out, int):\n",
    "                if out.shape[0] < size_thres or out.shape[1] < size_thres:\n",
    "                    continue\n",
    "#                 ls_crops.append(out)\n",
    "#                 tensor = transform_image(out)\n",
    "                # get prediction\n",
    "                stage = get_prediction(out)\n",
    "      \n",
    "                im = Image.fromarray(out)\n",
    "                im.save(os.path.join(save_path, stage, img_name) + '_' +  str(idx) + \".png\" )\n",
    "                \n",
    "                d_results[stage].append(idx)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Inference complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "#     f, ax = plt.subplots(figsize = (12, 12))\n",
    "#     ax.cla()\n",
    "#     # yellow: ring; magenta: troph; cyan: shiz\n",
    "\n",
    "#     colors_stage = { \"U\": [0, 0, 0], \"R\": [0.9, 0.9, 0], \n",
    "#         \"T\": [1, 0, 1], \"S\": [0, 1, 1]\n",
    "#     }\n",
    "#     ax.imshow(img)\n",
    "#     ax.axis('off')\n",
    "#     for k in class_names:\n",
    "#         if len(d_results[k]) > 0:\n",
    "#             for cell in d_results[k]:\n",
    "#                 coord = outlines_ls[cell]\n",
    "#                 ax.plot(coord[:,0], coord[:,1], color = colors_stage[k], lw=1.5)\n",
    "#     plt.show()\n",
    "#     out_stat = []\n",
    "#     for k in class_names:\n",
    "#         out_stat.append(len(d_results[k]))\n",
    "#     df = pd.DataFrame({ \"all_cells\":np.sum(out_stat), 'U':out_stat[0],\n",
    "#                 \"R\":out_stat[1], \"T\":out_stat[2], \"S\":out_stat[3], \n",
    "#                 \"Parasitemia\": 1 - out_stat[0]/np.sum(out_stat)}, index = [0])\n",
    "#     print(df)\n",
    "# #     print(\"Total cell count:\", df_res.sum().cell_idx)\n",
    "# #     print (\"Infected cells: \",df_res[df_res.stage != 'un'].sum().cell_idx)\n",
    "# #     print (\"Parasite density: \",df_res[df_res.stage != 'un'].sum()['paras'].round(4),'%')\n",
    "#     print('\\n')\n",
    "#     print('*'*10, 'Finished processing', '*'*10)\n",
    "#     print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-cherry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "outlines_ls = outlines_list(masks)\n",
    "with torch.no_grad():\n",
    "    for idx, cell in enumerate(outlines_ls[:]):\n",
    "        # get crop\n",
    "        out = get_crop_from_outline(cell, tmp_img)\n",
    "        if out.shape[0] < size_thres or out.shape[1] < size_thres:\n",
    "#             print(out.shape)\n",
    "            continue\n",
    "        tensor = transform_image(out)\n",
    "        # get prediction\n",
    "        stage = get_prediction(out)\n",
    "        d_results[stage].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cell_sizes\n",
    "np.std(data, ddof=1) / np.sqrt(np.size(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cell_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-illinois",
   "metadata": {},
   "source": [
    "#### Approach 2: find objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not crop on cell boundaries, incrrect inference\n",
    "\n",
    "# %%timeit\n",
    "# from cellpose\n",
    "size_threshold = 50 # in pix\n",
    "slices = find_objects(masks)\n",
    "tmp_img = img.copy()\n",
    "ls_img = []\n",
    "ls_outlines = []\n",
    "for i, si in enumerate(slices):\n",
    "    if si is not None:\n",
    "        x_start, x_stop = si[0].start, si[0].stop\n",
    "        y_start, y_stop = si[1].start, si[1].stop\n",
    "        if x_stop - x_start < size_threshold or y_stop - y_start < size_threshold:\n",
    "            continue\n",
    "        # get cell outlines\n",
    "        sr,sc = si\n",
    "        mask = (masks[sr, sc] == (i+1)).astype(np.uint8)\n",
    "        contours = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        pvc, pvr = np.concatenate(contours[-2], axis=0).squeeze().T  \n",
    "        ls_outlines.append((pvc+y_start, pvr+x_start))    \n",
    "        # get cell crops\n",
    "        out = tmp_img[si]\n",
    "        out = out[10:-10, 10:-10, :]\n",
    "        tensor_img = transform_image(out)\n",
    "        ls_img.append(tensor_img)\n",
    "\n",
    "img_tensor = torch.cat(ls_img, dim=0)\n",
    "bs = 128\n",
    "ls_preds = []\n",
    "part = img_tensor.shape[0]//bs\n",
    "for p in range(part+1):\n",
    "    outputs = model.forward(img_tensor[bs*p:bs*p + bs])\n",
    "    ls_preds.append(outputs.data.numpy().argmax(1))\n",
    "ar_preds = np.concatenate(ls_preds)\n",
    "results = [class_names[x] for x in ar_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cell_x, cell_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-asian",
   "metadata": {},
   "source": [
    "#### Approach 4: masks and find_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "objects = find_objects(masks)\n",
    "ls_img = []\n",
    "ls_outlines = []\n",
    "# get crops with precise outlines\n",
    "for n in range(masks.max()):\n",
    "    mn = np.array((masks==(n+1))*255, dtype = np.uint8)\n",
    "#     print(np.sum(mn>0))\n",
    "    cell_mask = np.repeat(mn[:, :, np.newaxis], 3, axis=2)\n",
    "    masked_image = cv2.bitwise_and(tmp_img, cell_mask)\n",
    "    cell_pix = objects[n]\n",
    "    cell_roi = masked_image[cell_pix]\n",
    "    # remove small particles\n",
    "    if cell_roi.shape[0] < 50 or cell_roi.shape[1] < 50:\n",
    "        continue\n",
    "    tensor_img = transform_image(cell_roi)\n",
    "    ls_img.append(tensor_img)\n",
    "    \n",
    "    # get cell outlines\n",
    "#     xrange, yrange = cell_pix\n",
    "#     x_start = xrange.start\n",
    "#     y_start = yrange.start\n",
    "#     mask = (masks[xrange, yrange] == (n+1)).astype(np.uint8)\n",
    "    contours = cv2.findContours(mn, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    out_pix_y, out_pix_x = np.concatenate(contours[-2], axis=0).squeeze().T  \n",
    "    # shift to the proper posiiton on the image\n",
    "    ls_outlines.append((out_pix_y, out_pix_x))   \n",
    "\n",
    "img_tensor = torch.cat(ls_img, dim=0)\n",
    "bs = 128\n",
    "ls_preds = []\n",
    "part = img_tensor.shape[0]//bs\n",
    "for p in range(part+1):\n",
    "    outputs = model.forward(img_tensor[bs*p:bs*p + bs])\n",
    "    ls_preds.append(outputs.data.numpy().argmax(1))\n",
    "preds = np.concatenate(ls_preds)\n",
    "results = [class_names[x] for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-karma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(out_pix_y, out_pix_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_stage = { \"un\": [1, 0, 0], \"ring\": [1, 1, 0], \n",
    "    \"troph\": [1, 0, 1], \"shiz\": [0, 1, 1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-chosen",
   "metadata": {},
   "source": [
    "#### Plotting from utils.outlines_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-camping",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (12, 12))\n",
    "# yellow: ring; magenta: troph; cyan: shiz\n",
    "\n",
    "colors_stage = { \"un\": [0, 0, 0], \"ring\": [0.9, 0.9, 0], \n",
    "    \"troph\": [1, 0, 1], \"shiz\": [0, 1, 1]\n",
    "}\n",
    "ax.imshow(imgout)\n",
    "ax.axis('off')\n",
    "for k in class_names:\n",
    "    if len(d_results[k]) > 0:\n",
    "        for cell in d_results[k]:\n",
    "            coord = outlines_ls[cell]\n",
    "            ax.plot(coord[:,0], coord[:,1], color = colors_stage[k], lw=1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame.from_dict(d_results, orient = 'index').stack().reset_index()\n",
    "df_res.columns = ['stage', '_' ,'cell_idx']\n",
    "del df_res[\"_\"]\n",
    "df_res = df_res.groupby('stage').count().reset_index()\n",
    "df_res['paras'] = df_res.cell_idx/total_count*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = sum(len(v)for v in d_results.values())\n",
    "print (f\"Final cell count: {total_count}\")\n",
    "print(\"Type    Count    %\")\n",
    "for key, value in d_results.items():\n",
    "    stage_count = len([item for item in value if item])\n",
    "    print(f\"{key}:    {stage_count}   {stage_count/total_count*100:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-macedonia",
   "metadata": {},
   "source": [
    "#### Plotting using find_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_cells = np.where(ar_preds > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (12, 12))\n",
    "\n",
    "ax.imshow(img)\n",
    "for idx in inf_cells[0]:\n",
    "    ax.plot(ls_outlines[idx][0], ls_outlines[idx][1], color = colors_stage[results[idx]], lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_results = np.array(results)\n",
    "total_cells = arr_results.size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-bradley",
   "metadata": {},
   "source": [
    "### Batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-stable",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "ls_img = []\n",
    "for idx, cell in enumerate(outlines_ls[:]):\n",
    "    \n",
    "    x = cell.flatten()[::2]\n",
    "    y = cell.flatten()[1::2]\n",
    "\n",
    "    if (y.max() - y.min()) < size_thres or (x.max() - x.min()) < size_thres:\n",
    "        continue\n",
    "\n",
    "    # mask outline\n",
    "    mask = np.zeros(img.shape, dtype=np.uint8)\n",
    "    channel_count = tmp_img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "    ignore_mask_color = (255,)*channel_count\n",
    "    # fill contour\n",
    "    cv2.fillConvexPoly(mask, cell, ignore_mask_color)\n",
    "\n",
    "    masked_image = cv2.bitwise_and(tmp_img, mask)\n",
    "\n",
    "    # crop the box around the cell\n",
    "    (topy, topx) = (np.min(y), np.min(x))\n",
    "    (bottomy, bottomx) = (np.max(y), np.max(x))\n",
    "    out = masked_image[topy:bottomy+1, topx:bottomx+1,:]\n",
    "#     out_scaled = cv2.resize(out, (224, 224), interpolation = cv2.INTER_AREA)\n",
    "    tensor = transform_image(out)\n",
    "    ls_img.append(tensor)\n",
    "\n",
    "img_tensor = torch.cat(ls_img, dim=0)\n",
    "bs = 128\n",
    "ls_preds = []\n",
    "part = img_tensor.shape[0]//bs\n",
    "for p in range(part+1):\n",
    "    outputs = model.forward(img_tensor[bs*p:bs*p + bs])\n",
    "    ls_preds.append(outputs.data.numpy().argmax(1))\n",
    "pred_ar = np.concatenate(ls_preds)\n",
    "results = [class_names[x] for x in pred_ar] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-deviation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-definition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_ap",
   "language": "python",
   "name": "pytorch_ap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
